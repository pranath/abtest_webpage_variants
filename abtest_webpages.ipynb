{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An analysis of A/B test results of web page variations\n",
    "\n",
    "This data analysis examines website traffic data from 2 web page variations to try to determine which page (if any) performs better with respect to conversions.\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Initial investigation](#initial)\n",
    "- [Part II - Manual A/B Test](#probability)\n",
    "- [Part III - Built-in function A/B Test](#ab_test)\n",
    "- [Part IV - Regression based A/B Test](#regression)\n",
    "- [Conclusion](#conclusion)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "In this project, I will be working to understand the results of an A/B test run by an e-commerce website. This test concerns a particular web page (old version) and a new version of the page (new design etc). My goal is to help the company understand if they should implement the new page, or keep the old page.\n",
    "\n",
    "I will be using 3 different approaches to perform the A/B test:\n",
    "\n",
    "* Manually calculated A/B test\n",
    "* A/B test using bulit-in library functions\n",
    "* A/B test using a regression based approach\n",
    "\n",
    "I will then compare and contrast the results of each, and arrive at a final reccomendation as to if the company should stick with the old page, or implement the new page.\n",
    "\n",
    "<a id='initial'></a>\n",
    "### Part I - Initial investigation\n",
    "\n",
    "Let's import our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Lets set a random seed so results are reproducable\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the dataset of our website traffic observations for the two page variants and take a look at the top few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a range of columns/variables, the group column indicates for which page variant that observation was for: control indicates the old page varient, treatment indicates the new page variant. The converted column indicates if the user converted or not - 1 = converted, 0 = did not convert. (We will leave out the specifics of what conversion actually means but assume it to be some common metric e.g. clicked through to a particular link on that page).\n",
    "\n",
    "#### Data wrangling/cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294478\n"
     ]
    }
   ],
   "source": [
    "total_rows = df.shape[0]\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290584\n"
     ]
    }
   ],
   "source": [
    "unique_users_count = df.user_id.nunique()\n",
    "print(unique_users_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is the proportion of users converted for both pages summed up together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11965919355605512\n"
     ]
    }
   ],
   "source": [
    "users_converted = df.converted.sum()\n",
    "prop_users_converted = users_converted / total_rows\n",
    "print(prop_users_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we need to check that for all observations regarding the group & landing_page columns:\n",
    "\n",
    "* 'control' is always associated with 'old_page' \n",
    "* 'treatment' is always associated with 'new_page'\n",
    "\n",
    "Lets check the number of times the `new_page` and `treatment` don't line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_newpage_treatment = df[(((df.group == 'treatment') & (df.landing_page != 'new_page')) | ((df.group != 'treatment') & (df.landing_page == 'new_page')))]\n",
    "no_newpage_treatment.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have a few, we will need to think about how we deal with these.\n",
    "\n",
    "Lets also check if any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_newpage_treatment.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok thats great at least no missing values!\n",
    "\n",
    "So for the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this observation truly received the new or old page.  \n",
    "\n",
    "So for our tests to be based on the most reliable data, we will remove these rows before we do our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(((df.group == 'treatment') & (df.landing_page == 'new_page')) | ((df.group == 'control') & (df.landing_page == 'old_page')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets see how many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290584\n"
     ]
    }
   ],
   "source": [
    "unique_users_count2 = df2.user_id.nunique()\n",
    "print(unique_users_count2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets check to see if we have any duplicated users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df2[df2.duplicated(['user_id'])]\n",
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we seem to have duplicated users.\n",
    "\n",
    "What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[(df2.user_id == 773192)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok apart from the different timestamp, both of these observations are essentially the same with respect to group, landing_page & conversion.\n",
    "\n",
    "Lets remove **one** of the rows with a duplicate **user_id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop_duplicates(subset=['user_id'], keep='first')\n",
    "df2[(df2.user_id == 773192)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Analysis\n",
    "\n",
    "Ok now that the data wrangling/cleaning is done - lets re-calculate the probability of an individual converting regardless of the page they receive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11959708724499628\n"
     ]
    }
   ],
   "source": [
    "total_rows2 = df2.shape[0]\n",
    "users_converted2 = df2.converted.sum()\n",
    "prob_user_conversion = users_converted2 / total_rows2\n",
    "print(prob_user_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1203863045004612\n"
     ]
    }
   ],
   "source": [
    "control = df2[df2['group'] == 'control']\n",
    "control_converted = control.converted.sum()\n",
    "prob_control_conversion = control_converted / control.shape[0]\n",
    "print(prob_control_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11880806551510564\n"
     ]
    }
   ],
   "source": [
    "treatment = df2[df2['group'] == 'treatment']\n",
    "treatment_converted = treatment.converted.sum()\n",
    "prob_treatment_conversion = treatment_converted / treatment.shape[0]\n",
    "print(prob_treatment_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our observations of the traffic to each of the two pages, lets calculate the observed difference in conversion between the two page variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0015782389853555567\n"
     ]
    }
   ],
   "source": [
    "# Diff as new - old page\n",
    "obs_diff_conversion = prob_treatment_conversion - prob_control_conversion\n",
    "print(obs_diff_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is the probability that an individual received the new page v the old page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000619442226688\n"
     ]
    }
   ],
   "source": [
    "got_new_page = df2[df2['landing_page'] == 'new_page']\n",
    "prob_new_page = got_new_page.shape[0] / df2.shape[0]\n",
    "print(prob_new_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Given that the probablity of conversion of both pages is so close, this suggests there is  NOT much evidence that one page leads to more conversions than the other, however we would want to do further statistical tests to be more sure of this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a>\n",
    "### Part II - Manual A/B Test\n",
    "\n",
    "Lets set up our null and alternate hypotheses for the test. \n",
    "\n",
    "So we want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%. So given **$p_{old}$** is the conversion rate of the old page, and **$p_{new}$** is the conversion rate of the new page, we will assume the following hypotheses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_0: p_{old} - p_{new} >= 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_1: p_{old} - p_{new} < 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we will assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, we will assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "We will use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Then we will create a sampling distribution to simulate the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null using bootstrapping. 10,000 iterations should be sufficient for the law of large numbers & the central limit theorem to apply i.e. for the distribution to be normally distributed, and to enable it to better estimate the actual difference in conversion between the pages for all website users i.e. our parameter from our confidence interval. <br><br>\n",
    "\n",
    "After simulating observations that would possible be at the top edge of the null, we will then compare this to the actual observed difference, and calculate a p-value to determine how likely our observation came from the null hypothesis. In other words, our p-value will help us determine if there is a statisically significant difference in the conversion rate of our two pages i.e. if the new page is **really** better than the old page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets recap what is the **convert rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_users_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what is the **convert rate** for $p_{old}$ under the null? <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11965919355605512"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_users_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define $n_{new}$, the number of observations that should be simulated for the new page under the null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145310\n"
     ]
    }
   ],
   "source": [
    "n_new = treatment.shape[0]\n",
    "print(n_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets define $n_{old}$, the number of observations that should be simulated for the old page under the null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145274\n"
     ]
    }
   ],
   "source": [
    "n_old = control.shape[0]\n",
    "print(n_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a test to see what values we can expect under the simulation, lets simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_page_converted = np.random.binomial(1, prop_users_converted, n_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_page_converted = np.random.binomial(1, prop_users_converted, n_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what type of values can we expect for the difference in conversion under our simulation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000745556951710899"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_new = new_page_converted.sum() / new_page_converted.shape[0]\n",
    "p_old = old_page_converted.sum() / old_page_converted.shape[0]\n",
    "p_new - p_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how this compares with the observed difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_diff_conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok these seem comparable, lets go ahead with the full simulation.\n",
    "\n",
    "We will now create a sampling distribution for our statistic (the difference in conversion rate between old & new pages) under the null hypothesis.\n",
    "\n",
    "Simulate 10,000 $p_{new}$ - $p_{old}$ values using this same process above.  Store all 10,000 values in **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bootstrap sampling to create 10,000 sample differences using the observed probability of conversion\n",
    "# for both pages combined i.e. under the null hypothesis, and then calculate 10,000 differences for each of these\n",
    "# (Use numpy built-in rather than for loop for more efficiency)\n",
    "new_converted_simulation = np.random.binomial(n_new, p_new,  10000)/n_new\n",
    "old_converted_simulation = np.random.binomial(n_old, p_old,  10000)/n_old\n",
    "# Diff as new - old page (keep same way of calculating difference as observed diff)\n",
    "p_diffs = new_converted_simulation - old_converted_simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the **p_diffs** - our sampling distribution of the diffence in conversion under the null hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs)\n",
    "# Also plot the actual observed difference in conversion in our sample\n",
    "plt.axvline(x=obs_diff_conversion, color = 'red'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems like our observed difference is well within the bulk of the sampling distribution of the simulated differences under the null hypothesis - i.e. it seems like our observed difference could easily have occured under the null hypothesis.[mention distribution does not include zero but very small]\n",
    "\n",
    "Lets calculate a p value to illustrate this i.e. what proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46\n"
     ]
    }
   ],
   "source": [
    "p_value = (p_diffs > obs_diff_conversion).mean()\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So our p value is the probability of observing our statistic (in this case the observed difference in conversion) given the null hypothesis is true. So if p is large, this means that the null is more likely to have generated our observed statistic ie that the null hypothesis is true. If p is small, this means that the null less likely to have generated our observed statistic. Also for the alternate hypothesis to be supported, we need a p value smaller than the type 1 error rate (alpha) of 5% (0.05), for a 95% confidence interval. Given our p value is larger than the alpha - therefore we failed to reject the null hypothesis. For our pages what this means is there is no significant statistical difference between the conversion rates of the old and new pages based on this evidence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "\n",
    "### Part III - Built-in function A/B Test\n",
    "\n",
    "As an alternative to our manually calculated test previously, more commonly we would use a built-in library function to achieve similar results more conveniently.  \n",
    "\n",
    "We will use the same null and alternate hypothesis declared in the manual approach i.e.\n",
    "\n",
    "$$H_0: p_{old} - p_{new} >= 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_1: p_{old} - p_{new} < 0$$\n",
    "\n",
    "Lets import new libraries needed for this built-in test, and segment our original observations by page type to feed directly into the test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = control_converted\n",
    "convert_new = treatment_converted\n",
    "n_old = control.shape[0]\n",
    "n_new = treatment.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now use `stats.proportions_ztest` to compute our test statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3109241984234394\n",
      "0.9050583127590245\n"
     ]
    }
   ],
   "source": [
    "# Function defaults to 2-sided test, but we want a 1-sided test ie where\n",
    "# alternative hypothesis is new_page > old_page therefore we must\n",
    "# pass extra parameter to function to ensure 1-sided test\n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new], alternative='smaller')\n",
    "print(z_score)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the z-score is a measure of the significance of the test, and given we are aiming for a 5% error rate this will mean a z-score less than -1.96 or greater than 1.96 will be signficiant. So the z-score is actually within these bounds, and while the p-value is a bit different from my manually computed in the previous section, it is still large enough to indicate that we failed to reject the null hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part IV - A regression approach\n",
    "\n",
    "So we can also do an A/B test by performing regression.\n",
    "\n",
    "Given the variable we want to predict for each page has two states (i.e. converted: 1 & 0), logistic regression would be a relevant regression technique we can use in this case.\n",
    "\n",
    "To convert this into a regression problem we need to think about what we are trying to do a little differently. In regression terms, we want to see how well we can fit a regression model to see if there is a significant difference in conversion based on which page a customer receives.\n",
    "\n",
    "Let's import the new library needed and prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>new_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  new_page  \n",
       "0          1         0  \n",
       "1          1         0  \n",
       "2          1         1  \n",
       "3          1         1  \n",
       "4          1         0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# We need to add an intercept column for the regression model base case\n",
    "df2['intercept'] = 1\n",
    "# Lets also convert the group column (page type) column into a dummy (0/1) variable \n",
    "# so our regression model can use it\n",
    "df2[['old_page', 'new_page']] = pd.get_dummies(df2['group'])\n",
    "# We don't need one of these new columns because the information is implicit in the other and\n",
    "# so the dropped column (old_page) will act as the baseline of the model \n",
    "df2 = df2.drop(['old_page'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as well as the intercept, we now have a new dummy variable column 'ab_page' which is 0 if its the old page, and 1 if the new page.\n",
    "\n",
    "Lets now use **statsmodels** to import our regression model.  \n",
    "\n",
    "Instantiate the model, and fit the model using the two columns created to see if the new page is significant in helping us predict conversions (compared to the baseline i.e. the old page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "logit_mod = sm.Logit(df2['converted'], df2[['intercept','new_page']])\n",
    "results = logit_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see a summary of our created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 26 Aug 2018</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>15:50:13</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new_page</th>  <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sun, 26 Aug 2018   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        15:50:13   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "new_page      -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the adjusted correlation coefficient for new_page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.015113064615719"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(-0.0150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our correlation coefficient indicates that for every 1 unit decrease on the new page, conversion is 1.01 times as likely as the old page - holding all else constant. In other words - there is very little difference in the conversion rate between the two pages.\n",
    "\n",
    "The p value is 0.19 - indicating that the new page is not significant in helping to predict conversion compareed to the old page. But is this a different p value compared to the p value calculated in the manual and built-in appoaches previously? (in terms of what the p value actually means)\n",
    "\n",
    "In the manual and built-in A/B tests, the null and alternative hypotheses were the following:\n",
    "\n",
    "$$H_0: p_{old} - p_{new} >= 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H_1: p_{old} - p_{new} < 0$$\n",
    " \n",
    "The p values in these cases was the probability of observing our statistic given the null hypothesis, ie the probability that the conversion rate of the old page is equal to OR BETTER THAN the new page. \n",
    "\n",
    "When using logistic regression, it would be more appropriate to use the following null & alternate hypotheses:\n",
    "\n",
    "$$H_0: p_{old} - p_{new} = 0$$\n",
    "\n",
    "$$H_1: p_{old} - p_{new} \\neq 0$$\n",
    " \n",
    "These are different hypotheses. Given the p value is the probability of observing our statistic given the null hypothesis, in this case the p value (of the new page) represents the probability that if the new page is used, that the user will convert, or alternatively, if using the new page is significant in predicting conversion compared to the baseline (the old page). These are different probabilities, and explains why the meaning as well as values of p values are different. Here the null hypthothesis is: both pages convert with an equal conversion rate. The p value of 0.19 for the new page indicates that there is no significant difference in the conversion rate of the new page i.e. that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more predictor variables may help us gain a more accurate prediction. However there are disadvantages to this that need to be considered, for example if some of our predictor variables are correlated with each other as well as with the response variable (conversion), if this was the case - it may for example effect the correlation coefficients of these variables in ways that make them difficult to interpret. It may also mean that our hypothesis testing results may not be reliable. This situation is also known as multicolliniearity. To determine if this is the case, we could use for example the pairplot function from the seaborn library on our dependant variables of choice. This plots all the possible combinations of these variables as pair-wise plots (e.g. using scatterplots, histograms) and allows us to more easily see if any of these variables are correlated with each other. Alternatively, we could calculate VIF's for these variables to see if any were significantly correlated with each other. If we find multicolliniearity, one simple solution would be to simply remove one of these co-depedant variables (the one of least interest) when we create our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a additional data set of countries for each of our observations (associated by user_id to the current data).\n",
    "\n",
    "Let's merge these country features into our current data, and see if country has an impact in predicting conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df = pd.read_csv('countries.csv')\n",
    "countries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df.country.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge in our country column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>new_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  new_page country  \n",
       "0          1         0      US  \n",
       "1          1         0      US  \n",
       "2          1         1      US  \n",
       "3          1         1      US  \n",
       "4          1         0      US  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.join(countries_df.set_index('user_id'), on='user_id')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's know create our dummy columns for the country column, and drop one which will act as our baseline (we will drop Canada/CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>new_page</th>\n",
       "      <th>country</th>\n",
       "      <th>country_uk</th>\n",
       "      <th>country_us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  new_page country  country_uk  country_us  \n",
       "0          1         0      US           0           1  \n",
       "1          1         0      US           0           1  \n",
       "2          1         1      US           0           1  \n",
       "3          1         1      US           0           1  \n",
       "4          1         0      US           0           1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[['country_ca', 'country_uk', 'country_us']] = pd.get_dummies(df2['country'])\n",
    "df2 = df2.drop(['country_ca'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a new regression model, using countries as well as the page to see if this can help us predict conversion better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 26 Aug 2018</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>15:50:15</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>   -2.0300</td> <td>    0.027</td> <td>  -76.249</td> <td> 0.000</td> <td>   -2.082</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_uk</th> <td>    0.0506</td> <td>    0.028</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>country_us</th> <td>    0.0408</td> <td>    0.027</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.012</td> <td>    0.093</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Sun, 26 Aug 2018   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        15:50:15   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0300      0.027    -76.249      0.000      -2.082      -1.978\n",
       "new_page      -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "country_uk     0.0506      0.028      1.784      0.074      -0.005       0.106\n",
       "country_us     0.0408      0.027      1.516      0.130      -0.012       0.093\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_mod2 = sm.Logit(df2['converted'], df2[['intercept','new_page','country_uk','country_us']])\n",
    "results2 = logit_mod2.fit()\n",
    "results2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the adjusted correlation coefficients of the countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0519020483004984, 1.0416437559600236)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.0506), np.exp(0.0408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our correlation coefficient's indicate that for every 1 unit increase in each country, conversion is 1.05 (UK) and 1.04 (US) times as likely - holding all else constant. In other words - there is very little difference in the conversion rate between users from the two countries.\n",
    "\n",
    "Given that the p values of the countries are not smaller than the type 1 error rate, this also suggests the countries are not a significant predictor for conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now looked at the individual factors of country and page on conversion, now lets look at an interaction between page and country to see if there significant effects on conversion.\n",
    "\n",
    "Let's create the necessary additional columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>new_page</th>\n",
       "      <th>country</th>\n",
       "      <th>country_uk</th>\n",
       "      <th>country_us</th>\n",
       "      <th>new_page_uk</th>\n",
       "      <th>new_page_us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  new_page country  country_uk  country_us  new_page_uk  \\\n",
       "0          1         0      US           0           1            0   \n",
       "1          1         0      US           0           1            0   \n",
       "2          1         1      US           0           1            0   \n",
       "3          1         1      US           0           1            0   \n",
       "4          1         0      US           0           1            0   \n",
       "\n",
       "   new_page_us  \n",
       "0            0  \n",
       "1            0  \n",
       "2            1  \n",
       "3            1  \n",
       "4            0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct new columns for all country/page combinations \n",
    "df2['new_page_uk'] = df2['new_page'] * df2['country_uk']\n",
    "df2['new_page_us'] = df2['new_page'] * df2['country_us']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create and fit a new model using these combined page/country features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366117\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290581</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sun, 26 Aug 2018</td> <th>  Pseudo R-squ.:     </th>  <td>1.082e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>15:50:19</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.3164</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>   <td>   -1.9926</td> <td>    0.008</td> <td> -252.910</td> <td> 0.000</td> <td>   -2.008</td> <td>   -1.977</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new_page_uk</th> <td>    0.0112</td> <td>    0.018</td> <td>    0.626</td> <td> 0.532</td> <td>   -0.024</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>new_page_us</th> <td>   -0.0144</td> <td>    0.012</td> <td>   -1.155</td> <td> 0.248</td> <td>   -0.039</td> <td>    0.010</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290581\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 26 Aug 2018   Pseudo R-squ.:               1.082e-05\n",
       "Time:                        15:50:19   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.3164\n",
       "===============================================================================\n",
       "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "intercept      -1.9926      0.008   -252.910      0.000      -2.008      -1.977\n",
       "new_page_uk     0.0112      0.018      0.626      0.532      -0.024       0.046\n",
       "new_page_us    -0.0144      0.012     -1.155      0.248      -0.039       0.010\n",
       "===============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_mod3 = sm.Logit(df2['converted'], df2[['intercept','new_page_uk','new_page_us']])\n",
    "results3 = logit_mod3.fit()\n",
    "results3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the adjusted correlation coefficients of the country/newpage combined features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.011262954811771, 1.0145041794607625)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(0.0112), 1/np.exp(-0.0144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It would seem that the combination of new page and countries are not significant predictors of conversion rate (p values far bigger than alpha of 0.05, correlation coefficients of approximately 1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "So all 3 of our techniques of A/B testing suggested there was no significant statistical difference in the conversion rate between the old and new pages i.e. that we failed to reject the null hypothesis. Based on this evidence, we might well reccomend there was no added value in the company deploying the new page variation and they should stick with the old page.\n",
    "\n",
    "(Explain - Manual method good as in some cases we may wish to estimate a parmeter from a statistic where the central limit theorem does not apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
